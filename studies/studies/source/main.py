# Imports from external packages
from absl import app
from absl import flags
import re
import os

# Imports of internal packages
from documentation import logger
from information.networking import requests
from scraper import scraper

# Setup flags, so the user can set up required arguments
FLAGS = flags.FLAGS
flags.DEFINE_string("target", None, "Target Location, either IP or Domain", required=True)
flags.DEFINE_string("logs", "../documentation/results.json", "Location for the result log. You can set a path for it, "
                                                            "or the default will be used.")



# checkFlags checks, whether all the necessary flags are set and will return true or false.
def checkFlags():
    if "localhost" in FLAGS.target:
        print("target defined as: %s!" % FLAGS.target)
        return True
    if FLAGS.target == "":
        print("No target was set, please try again")
        return False
    print("target defined as: %s!" % FLAGS.target)
    return True


# main is the Main-Function that is the starting point of the program
def main(argv):
    url = FLAGS.target
    os.environ['TARGET'] = str(url)
    print(os.environ['TARGET'])
    if checkFlags():
        logger.log("Initiating logs for testing the website running at %s" % url, "Initiation")
        reachable, information = requests.ping(url)
        if reachable:
            logger.log(information, "Ping Website")
            logger.log("Starting to Scrape the Website using Scrapy", "Sraping Website")
            s = scraper.Spider(url)
            os.chdir("studies/")
            os.system('scrapy crawl %s -o %s' % (s.name, FLAGS.logs))
        else:
            logger.error("Ping Website", information)


if __name__ == "__main__":
    app.run(main)